%% Lee
%% In dissertation, change 
%    section* to chapter 
%    subsection* to section
%    subsubsection* to subsection

% >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> DRAM Customizable <<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<
\chapter{DRAM Overview and Customizations}
\label{sec:DRAM Customizations}

There are two types of memory employed in \acp{asic} and \acp{asip}, \acf{sram} and \acf{dram}.
Both of these technologies have a similar top level block diagram which contains an array of storage elements, a means to address into a particular row of memory cells and a means to read and write a column of those cells.
A basic block diagram is shown in figure \ref{fig:MemoryBlockDiagram}.
\begin{figure}[!t]
% the [] contains position info e.g. [!t] means here
\centering
\captionsetup{justification=centering}
\centerline{
\mbox{\includegraphics[width=.8\linewidth]{BasicMemoryBlockDiagram}}
}
\caption{Typical Memory Block Diagram \cite{Jacob:2007:MSC:1543376}}
\label{fig:MemoryBlockDiagram}
\end{figure}


\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[scale=0.4]{SRAM_Cell}
  \captionsetup{justification=centering, skip=5pt}
  \vspace{-6pt}
  \caption{SRAM Storage Cell \cite{Jacob:2007:MSC:1543376}}
  \label{fig:SRAM Cell}
\end{subfigure}%
\begin{subfigure}{.45\textwidth}
  \centering
  \includegraphics[scale=0.4]{DRAM_Cell}
  \captionsetup{justification=centering, skip=5pt}
  %\vspace{36pt}
  \vspace{20pt}
  \caption{DRAM Storage Cell \cite{Jacob:2007:MSC:1543376}}
  \label{fig:DRAM Cell}
\end{subfigure}
\captionsetup{justification=centering, skip=12pt}
\caption{RAM Storage Cell Types}
\label{fig:Memory Storage Cells}
\end{figure}

The major difference between the \ac{sram} cell and \ac{dram} cell is the number of elements it takes to implement. 
The \ac{sram} cell takes six transistors and the \ac{dram} cell takes one transistor and one capacitor.
The size of the \ac{dram} cell means \ac{dram} arrays provide five to six times more storage density when compared to a similar sized \ac{sram} array.

The major disadvantage is the capacitor cannot hold a indefinitely because the leakage currents in \acp{ic} cause the charge to leak away. 
If kept unchecked, the stored value will dissipate and it is this behavior that makes accessing a \ac{dram} array more complicated than a similar \ac{sram} array.

\section{Accessing \ac{dram} and \ac{sram}}
\label{Accessing DRAM and SRAM}

Accessing a typical \ac{sram} involves providing an address and either reading or writing the contents of that location. 
The read or write often completes in one or two clock cycles depending on whether the \ac{sram} employs internal registers which are used run the \ac{sram} with a faster clock.

The storage cell inside the \ac{sram} is formed from cross-coupled transisters (see figure \ref{fig:SRAM Cell}) which latch the contents and hold the contents indefnitely or until power is removed from the device.
The storage structure employs six transistors and allows the access logic to be relatively simple and fast but has a relatively low density because of the number of transistors employed.

Accessing a "typical" \ac{dram} is much more involved, it involves opening a page in a bank, reading or writing a portion of the contents of the page then closing the page. 

When an \ac{sram} cell is read, the cross-coupled transistors retain the stored value. 
The reasons behind this added complexity is the memory cell inside the \ac{dram} which is formed from a capacitor (see figure \ref{fig:DRAM Cell}) which holds a charge reflecting a logic zero or one. 
When a page(row) of a \ac{dram} memory array is read by the sense amps (see figure \ref{fig:dramBlockDiagram}), the process of sensing the charge on the capacitor causes the capacitor to discharge and lose its contents. 
To alleviate this problem \iffalse \ac{dram} arrays are formed from a column of storage elements known as a page. \fi when the read occurs, the entire contents of the page are transferred to registers, refered to as "page intermediate store" in figure \ref{fig:dramBlockDiagram}. 
The process of transferring a page to the intermediate store is known as a "page open".
Once this transfer is complete, portions of the open page can be read similar to reading an \ac{sram}. 

\iffalse
The value charged on the capacitors are transferred to the page intermediate store and all further reads and writes are to this intermediate store. 

When a different page is accessed, the contents of the page intermediate store has to be returned to the memory array and the capacitors recharged.
The new page can then be transferred to the intermediate store for accessing.
\fi

\iffalse The issue with \acp{dram} is when a storage cell is read, the capacitor is discharged. In addition, sensing this discharging capacitor takes a long time when compared to the \ac{sram} cell.\fi

The problem is that if another read wants to access data that is not in the page, the page has to be closed and another page opened. 
This involves transferring the previously registered page back to the array to recharge the capacitors in the memory array storage elements. 
The next page can then be opened and transferred to the page intermediate registers.

In practice, the \ac{dram} protocol is separated into "page" commands and "cache" commands. The page commands open and close pages and the cache commands read and write to the page intermediate store.

The process of opening and closing pages is a relatively long time, typically 10-20\SI[per-mode=symbol]{}{\nano\second}. Once a page is open, accessing the intermediate store is much faster.
So if the accesses are somewhat random and different pages are constantly accessed and pages are constantly being opened and closed, the average time to complete reads and writes are very large when compared to \ac{sram}.
To alleviate this issue, the \ac{dram} is formed from more than one array of storage elements, between 8 and 32, known as banks. The idea is that while a page from one bank is being accessed, another banks page can be opened in preparation for a future read (or write).
This access protocol is rather complicated and involves interleaving page commands and cache commands to multiple banks. 
The system memory controller logic must keep track of which pages are open inside the \ac{dram} and for each memory request must determine if a page needs to be closed and another opened before reading (or writing) the intermediate store.
If consequtive accesses are not sequenced carefully, the performance of \ac{dram} can be poor.


\subsection{Access Locality/Reuse and \ac{sram} as a Cache}
\label{Access Locality/Reuse and Cache}

In general purpose computing, the sequence of accesses cannot always be controlled, so \ac{sram} is typically used as the first level of memory with \ac{dram} used as the primary storage. 
Using \ac{sram} as this first level memory is called a cache and acts like a mirror of the \ac{dram} contents.
These caches have been used for decades to isolate the computing system from unpredictable access behavior of the \ac{dram}.
The general idea behind caches is that most data exhibits spatial and temporal locality. This "locality" means that when a computer program uses a piece of data in memory, it is very likely that soon after other data "close" to that data will be used and/or the same data will be reused.
So when a piece of data in memory is accessed, that data and a large block of data in close proximity to the requested data are transferred to the cache. 
The cache is designed to hold multiple of these blocks of data often resulting in tens of \SI[per-mode=symbol]{}{\kilo \byte} of \ac{sram}.
When other memory requests are made, the memory controller checks to see whether the block associated with the requested data is present in the cache.
If the requested data is contained in a block present in the cache, this is considered a "hit" and the data in the cache is used. 
If the requested data's block is not present, this is known as a "miss" and the slower main memory must be accessed. This access results in another "block" of data being transferred to the cache.
If all the blocks in the cache are currently fully employed, one of the blocks must be freed up to make space in the cache for the new block. 
A block is chosen and transferred back to the main memory and the new block is read and transferred to the cache.

To make effective use of the cache, the access behavior of the computer program must exhibit this locality behavior. 
If the cache blocks are large enough and the programs access behavior exhibit locality, then employing \ac{sram} is effective and the slower \ac{dram} access times can be somewhat hidden.

As mentioned in section \ref{sec:overview} much of the \ac{asic} and \ac{asip} \ac{ann} research has focused on taking advantage of the performance and ease of use of \ac{sram}. 
If the target application means the memory access behavior exhibits some locality and the \ac{sram} cache can be made large enough to avoid high levels of cache misses, then this use of \ac{sram} can be effective.

But this works target application is such that the access behavior exhibits no or little locality (reuse) and block transfers between the cache and main memory would be constantly occuring.
Under these circustances, \textbf{\textcolor{black}{\ac{dram} bandwidth will be the bottleneck}}.

This work focuses on using \ac{dram} as the primary storage and managing the accesses to ensure the \ac{dram} is used effectively. 
With the increased bandwidth achieved from the additional \ac{dram} customizations discussed in section \ref{sec:Very-Wide Bus} and \ref{sec:Write Mask}, this work demonstrates \ac{dram} bandwidths 10X faster than what is available with 2 or 2.5D solutions.

This high level of \ac{dram} bandwidth provides this work the ability to process multiple disparate \acp{ann} at or near real-time whilst being \textbf{\textcolor{black}{10-100X faster than state-of-the-art solutions}}.


\section{DRAM Customizations}

In a typical \ac{dram}, a bank may contain of the order of a few thousand pages and a page may contain of the order of a few thousand bits.
Once the page is open, the user accesses a portion of the requested page over a bus. With PCB based DRAMs the bus might vary from four to 16 bits wide, but with 3D DRAMs, such as HBM the bus might be up to 128 bits wide.
An \ac{asic}, \ac{asip} or \ac{gpu} implementation may combine multiple devices to generate bus widths of the order of \SI[per-mode=symbol]{1}{\kilo \bit} wide. 
When using 2.5D technology and \ac{hbm} with their Pascal\texttrademark \ac{gpu} accelerator device, NVidia\textregistered ~achieve a raw \ac{dram} bandwidth approaching \SI[per-mode=symbol]{6}{\tera \bit \per \second} \cite{Nvidia_p100_summary_datasheet}\footnote{datasheet also shows a \ac{tdp} of \SI[per-mode=symbol]{300}{\watt}}.
However, experience has shown \cite{farabet2011neuflow} \cite{tensorflow2015-whitepaper} that usable bandwidth will likely be much lower.
Regardless, this existing technology does not achieve the required bandwidth \eqref{eq:maximumBandwidth}.


%Considering systems will want to perform multiple \ac{dnn}s simultaneously suggests that these edge systems will require usable memory bandwidth of the order of 10s of \SI[per-mode=symbol]{}{\tera \bit \per \second} \eqref{eq:maximumBandwidth}.

To achieve increased \ac{dram} bandwidth this work is proposing two changes to the Tezzaron\textregistered \ac{diram4} \cite{tezzaron:diram4} \ac{3d} \ac{dram}. 
The most significant customization is to widen the databuses to generate more raw bandwidth. This is discussed further in section \ref{sec:Very-Wide Bus}.

With the customizations discussed in sections \ref{sec:Very-Wide Bus} and \ref{sec:Write Mask}, this work demonstrates \ac{dram} bandwidths >10X faster than what is available with 2 or 2.5D solutions.

\subsection{Customization One: Very-Wide Bus}
\label{sec:Very-Wide Bus}

Figure \ref{fig:dramBlockDiagram} shows a block diagram of a typical DRAM.

\begin{figure}[!t]
% the [] contains position info e.g. [!t] means here
\centering
\captionsetup{justification=centering}
\centerline{
\mbox{\includegraphics[width=.75\linewidth]{DRAMBlockDiagram}}
}
\caption{Typical DRAM Block Diagram}
\label{fig:dramBlockDiagram}
\end{figure}

This work achieves the increase in bandwidth by proposing that the DRAM expose more of its currently open page.

Without the limitations of having to transfer data beyond the chip stack, this work suggests exposing a larger portion of the page over a very wide bus. By staying within the 3D footprint, this bus can be implemented using fine pitch through-silicon-vias.
(see figure \ref{fig:dramBusChange}).

\begin{figure}[!t]
% the [] contains position info e.g. [!t] means here
\centering
\captionsetup{justification=centering}
\captionsetup{width=.75\linewidth}
\centerline{
\mbox{\includegraphics[width=.65\linewidth]{DRAMBusChange}}
}
\caption{Exposing more of the DRAM page}
\label{fig:dramBusChange}
\end{figure}

This work assumes the \ac{dram} interface protocol uses \ac{ddr} with a bus width of 2048. Given the \ac{diram4} employs a burst of two for read and write cycles, an entire \ac{diram4} page of 4096-bits is accessed during each read or write. 

\subsection{Customization Two: Write Mask}
\label{sec:Write Mask}
When processing an ANN, to compute the activation of an individual AN involves reading the pre-synaptic AN activations and the weights of the connections between the pre-synaptic ANs and the AN being processed. The activation of the processed AN is written back to memory. The ratio of reads to writes is high, 100s or 1000s to one. Therefore, the system often needs to write a portion of the page back to memory. To avoid a read/modify/write, a customization to the DRAM is the addition of a write data mask to the DRAM write path.

This work assumes single precision floating point for \ac{ann} weights and activation, so a mask bit will be provided on a word basis or every 32-bits.

