@book{aizenberg2013multi,
  title={Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications},
  author={Aizenberg, Igor and Aizenberg, Naum N and Vandewalle, Joos PL},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{standard2007jedec,
  title={DDR3 SDRAM Standard},
  author={{Standard, JEDEC}},
  journal={JESD79-3, Jun},
  year={2007}
}


@manual{micron_ddr3,
    organization  = "Micron Technology Inc.",
    title         = "DDR3L SDRAM",
    number        = "D 8/17 EN",
    year          =  2015
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{liu2012compact,
  title={A compact low-power 3D I/O in 45nm CMOS},
  author={Liu, Yong and Luk, Wing and Friedman, Daniel},
  booktitle={2012 IEEE International Solid-State Circuits Conference},
  pages={142--144},
  year={2012},
  organization={IEEE}
}


@ARTICLE{patti2006,
author={R. S. Patti},
journal={Proceedings of the IEEE},
title={Three-Dimensional Integrated Circuits and the Future of System-on-Chip Designs},
year={2006},
volume={94},
number={6},
pages={1214-1224},
keywords={integrated circuit interconnections;system-on-chip;3D integrated circuits;integrated circuit interconnections;system-on-chip design;Capacitance;Computational geometry;Costs;Delay;Integrated circuit interconnections;Random access memory;System-on-a-chip;Three-dimensional integrated circuits;Wire;Wiring;Integrated circuit interconnections;three-dimensional integrated circuits (3-D ICs)},
doi={10.1109/JPROC.2006.873612},
ISSN={0018-9219},
month={June},}

@article{doi:10.1162/neco.2006.18.7.1527,
author = { Geoffrey E. Hinton and  Simon Osindero and  Yee-Whye Teh},
title = {A Fast Learning Algorithm for Deep Belief Nets},
journal = {Neural Computation},
volume = {18},
number = {7},
pages = {1527-1554},
year = {2006},
doi = {10.1162/neco.2006.18.7.1527},
note ={PMID: 16764513},
URL = {https://doi.org/10.1162/neco.2006.18.7.1527},
eprint = {https://doi.org/10.1162/neco.2006.18.7.1527},
abstract = { We show how to use âcomplementary priorsâ to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. }
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@misc{krizhevsky2012imagenetPreso,
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  howpublished = {\url{http://image-net.org/challenges/LSVRC/2012/supervision.pdf}},
  note = {Accessed: 2016-08-30}
}

@misc{stanford_softmax,
  title = {Softmax regression},
  author={Various},
  howpublished = {\url{http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/}},
  note = {Accessed: 2018-02-16}
}

@misc{stanford_pooling,
  title = {Pooling Overview},
  author={Various},
  howpublished = {\url{http://ufldl.stanford.edu/tutorial/supervised/Pooling/}},
  note = {Accessed: 2018-02-16}
}

@article{galal2011energy,
  title={Energy-efficient floating-point unit design},
  author={Galal, Sameh and Horowitz, Mark},
  journal={IEEE Transactions on Computers},
  volume={60},
  number={7},
  pages={913--922},
  year={2011},
  publisher={IEEE}
}

@misc{nvidia_gpi_comparison,
  title = {Nvidia Pushes Deep Learning Inference With New Pascal GPUs},
  author={Morgan, Timothy},
  howpublished = {\url{https://www.nextplatform.com/2016/09/13/nvidia-pushes-deep-learning-inference-new-pascal-gpus}},
  note = {Accessed: 2018-02-21}
}


@inproceedings{schabel2016processor,
  title={Processor-in-memory support for artificial neural networks},
  author={Schabel, Joshua and Baker, Lee and Dey, Sumon and Li, Weifu and Franzon, Paul D},
  booktitle={Rebooting Computing (ICRC), IEEE International Conference on},
  pages={1--8},
  year={2016},
  organization={IEEE}
}

@phdthesis{schabel2014diss,
  title={Design of an Application-Specific Instruction Set Processor for the Sparse Neural Network Design Space},
  author={Schabel, Joshua C.},
  year={2017},
  publisher={Electrical and Computer Engineering},
  organization={North Carolina State University},
  address={Box 7911, Raleigh, NC 27695-7911}
}

@techreport{schabel2017energy,
  title={Predictive energy-Per-Op scaling for exploring the design space},
  author={Schabel, Joshua C. and Park, Jong Beom and Davis, Rhett William and Franzon, Paul D.},
  year={2014},
  publisher={Electrical and Computer Engineering},
  organization={North Carolina State University},
  address={Box 7911, Raleigh, NC 27695-7911}
}


@manual{park2015data,
  title={Three-Dimensional DRAM Area, Timing and Energy Model},
  author={Park, Jong Beom},
  year={2015},
  school={Electrical and Computer Engineering},
  organization={North Carolina State University},
  address={Box 7911, Raleigh, NC 27695-7911}
}

@manual{tezzaron:diram4,
    organization  = "Tezzaron Semiconductor",
    title         = "DiRAM4-64Cxx Cached Memory Subsystem",
    number        = "",
    year          =  2015,
    month         =  "July",    
    note          = "Rev. 0.04"
}

%http://www.imapsource.org/action/showCitFormats?doi=10.4071%2F2014DPC-ta12
@article{patti2014,
author = { Robert  Patti },
organization  = "Tezzaron Semiconductor",
title = {2.5D and 3D Integration Technology Update},
journal = {Additional Conferences (Device Packaging, HiTEC, HiTEN, \& CICMT)},
volume = {2014},
number = {DPC},
pages = {1-35},
year = {2014},
doi = {10.4071/2014DPC-ta12},
URL = { 
        https://doi.org/10.4071/2014DPC-ta12
},
eprint = { 
        https://doi.org/10.4071/2014DPC-ta12
}
}


@article{qiu2013parallel,
  title={A parallel neuromorphic text recognition system and its implementation on a heterogeneous high-performance computing cluster},
  author={Qiu, Qinru and Wu, Qing and Bishop, Morgan and Pino, Robinson E and Linderman, Richard W},
  journal={IEEE Transactions on Computers},
  volume={62},
  number={5},
  pages={886--899},
  year={2013},
  publisher={IEEE}
}

@article{maddison2014move,
  title={Move evaluation in go using deep convolutional neural networks},
  author={Maddison, Chris J and Huang, Aja and Sutskever, Ilya and Silver, David},
  journal={arXiv preprint arXiv:1412.6564},
  year={2014}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{bojarski2016end,
  title={End to End Learning for Self-Driving Cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@article{lillo1994synthesis,
  title={Synthesis of Brain-State-in-a-Box (BSB) based associative memories},
  author={Lillo, Walter E and Miller, David C and Hui, Stefen and Zak, Stanislaw H},
  journal={IEEE transactions on Neural Networks},
  volume={5},
  number={5},
  pages={730--737},
  year={1994},
  publisher={IEEE}
}

@inproceedings{esmaeilzadeh2005nnsp,
  title={NnSP: embedded neural networks stream processor},
  author={Esmaeilzadeh, Hadi and Farzan, Farhang and Shahidi, Neda and Fakhraie, SM and Lucas, Caro and Tehranipoor, Mohammad},
  booktitle={48th Midwest Symposium on Circuits and Systems, 2005.},
  pages={223--226},
  year={2005},
  organization={IEEE}
}

@inproceedings{kim2016neurocube,
  title={Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory},
  author={Kim, Duckhwan and Kung, Jaeha and Chai, Sek and Yalamanchili, Sudhakar and Mukhopadhyay, Saibal},
  booktitle={Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on},
  pages={380--392},
  year={2016},
  organization={IEEE}
}

@inproceedings{gao2017tetris,
  title={Tetris: Scalable and efficient neural network acceleration with 3d memory},
  author={Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos},
  booktitle={Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={751--764},
  year={2017},
  organization={ACM}
}

@article{kyo2011imapcar,
  title={IMAPCAR: A 100 GOPS in-vehicle vision processor based on 128 ring connected four-way VLIW processing elements},
  author={Kyo, Shorin and Okazaki, Shin’ichiro},
  journal={Journal of Signal Processing Systems},
  volume={62},
  number={1},
  pages={5--16},
  year={2011},
  publisher={Springer}
}

@inproceedings{farabet2011neuflow,
  title={Neuflow: A runtime reconfigurable dataflow processor for vision},
  author={Farabet, Cl{\'e}ment and Martini, Berin and Corda, Benoit and Akselrod, Polina and Culurciello, Eugenio and LeCun, Yann},
  booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on},
  pages={109--116},
  year={2011},
  organization={IEEE}
}

@inproceedings{maas2013rectifier,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
  booktitle={Proc. ICML},
  volume={30},
  number={1},
  year={2013}
}

@inproceedings{gokhale2014240,
  title={A 240 g-ops/s mobile coprocessor for deep neural networks},
  author={Gokhale, Vinayak and Jin, Jonghoon and Dundar, Aysegul and Martini, Berin and Culurciello, Eugenio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={682--687},
  year={2014}
}

@article{azarkhish2017neurostream,
  title={Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes},
  author={Azarkhish, Erfan and Rossi, Davide and Loi, Igor and Benini, Luca},
  journal={arXiv preprint arXiv:1701.06420},
  year={2017}
}

@inproceedings{chen2014diannao,
  title={Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning},
  author={Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
  booktitle={ACM Sigplan Notices},
  volume={49},
  number={4},
  pages={269--284},
  year={2014},
  organization={ACM}
}

@article{chen2016diannao,
 author = {Chen, Yunji and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and Temam, Olivier},
 title = {DianNao Family: Energy-efficient Hardware Accelerators for Machine Learning},
 journal = {Commun. ACM},
 issue_date = {November 2016},
 volume = {59},
 number = {11},
 month = oct,
 year = {2016},
 issn = {0001-0782},
 pages = {105--112},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2996864},
 doi = {10.1145/2996864},
 acmid = {2996864},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@INPROCEEDINGS{dadiannao2014,
author={Y. Chen and T. Luo and S. Liu and S. Zhang and L. He and J. Wang and L. Li and T. Chen and Z. Xu and N. Sun and O. Temam},
booktitle={47th Annual IEEE/ACM International Symposium on Microarchitecture},
title={DaDianNao: A Machine-Learning Supercomputer},
year={2014},
volume={},
number={},
pages={609-622},
keywords={learning (artificial intelligence);mainframes;neural nets;parallel machines;CNN-DNN algorithmic characteristics;DaDianNao;GPU;computational capacity-area ratio;computational units;convolutional neural network;custom storage;deep neural network;general-purpose workloads;high-degree parallelism;industry-grade interconnects;machine-learning supercomputer;multichip machine-learning architecture;multichip system;neural network accelerators;Bandwidth;Biological neural networks;Computer architecture;Graphics processing units;Hardware;Kernel;Neurons;accelerator;computer architecture;machine learning;neural network},
doi={10.1109/MICRO.2014.58},
ISSN={1072-4451},
month={Dec},}

@ARTICLE{dadiannao2017,
author={T. Luo and S. Liu and L. Li and Y. Wang and S. Zhang and T. Chen and Z. Xu and O. Temam and Y. Chen},
journal={IEEE Transactions on Computers},
title={DaDianNao: A Neural Network Supercomputer},
year={2017},
volume={66},
number={1},
pages={73-88},
keywords={feedforward neural nets;graphics processing units;learning (artificial intelligence);multiprocessor interconnection networks;parallel machines;CNN;DNN;DaDianNao;GPU;computational capacity-area ratio;convolutional neural networks;custom multichip machine-learning architecture;deep neural networks;electrical interchip interconnects;general-purpose workloads;machine-learning algorithms;memory footprint;multichip system;neural network accelerators;neural network supercomputer;on-chip storage;optical interchip interconnects;Biological neural networks;Computer architecture;Graphics processing units;Hardware;Kernel;Neurons;Supercomputers;CNN;DNN;Machine learning;interconnect;multi-chip;neuron network;supercomputer},
doi={10.1109/TC.2016.2574353},
ISSN={0018-9340},
month={Jan},}

@inproceedings{chen201614,
  title={14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
  booktitle={2016 IEEE International Solid-State Circuits Conference (ISSCC)},
  pages={262--263},
  year={2016},
  organization={IEEE}
}


@manual{afrl_cortical, 
    organization  = "North Carolina State University",
    title         = "Hardware Acceleration of Sparse Cognitive Algorithms",
    author        = {Franzon, Paul D. and Baker, Lee and Dey, Sumon and Li, Weifu and Schabel, Joshua},
    number        = "FA8650-15-7518",
    year          =  2016,
    month         =  "March",    
    note          = "AFRL"
}


@misc{techpowerupteslak20c,
  title = {NVidia Tesla K20c},
  author={Techpowerup},
  howpublished = {\url{https://www.techpowerup.com/gpudb/564/tesla-k20c}},
  note = {Accessed: 2016-09-08}
}

@misc{Nvidia_p100_summary_datasheet,
  title = {NVidia Tesla P100 Datasheet},
  author={Nvidia\textregistered},
  howpublished = {\url{http://images.nvidia.com/content/tesla/pdf/nvidia-tesla-p100-datasheet.pdf}},
  note = {Accessed: 2017-12-29}
}

@misc{wccftechpascal,
  title = {Nvidia : Pascal Is 10x Faster Than Maxwell},
  author={WCCF Tech},
  howpublished = {\url{http://wccftech.com/nvidia-pascal-gpu-gtc-2015/}},
  note = {Accessed: 2016-09-08}
}

@misc{NNintro_Bullinaria,
  title = {Introduction to Neural Computation : Neural Computation},
  author={Dr John A. Bullinaria},
  howpublished = {\url{http://www.cs.bham.ac.uk/~jxb/inc.html}},
  note = {Accessed: 2017-09-08}
}

@misc{NNintro_Nielsen,
  title = {NNs and DL},
  author={Michael Nielsen},
  howpublished = {\url{http://www.neuralnetworksanddeeplearning.com/index.html}},
  note = {Accessed: 2018-01-02}
}
%  title = {Neural Networks and Deep Learning},

@misc{wikipedia_softmax,
  title = {Softmax function},
  author={Various},
  howpublished = {\url{https://en.wikipedia.org/wiki/Softmax_function}},
  note = {Accessed: 2018-01-02}
}

@misc{wikipedia_neuron,
  title = {Neuron},
  author={Various},
  howpublished = {\url{https://en.wikipedia.org/wiki/Neuron}},
  note = {Accessed: 2018-01-02}
}

@misc{wikipedia_deep_learning,
  title = {Neuron},
  author={Various},
  howpublished = {\url{https://en.wikipedia.org/wiki/Deep_learning}},
  note = {Accessed: 2018-01-02}
}

@misc{wikipedia_back_propagation,
  title = {Backpropagation},
  author={Various},
  howpublished = {\url{https://en.wikipedia.org/wiki/Backpropagation}},
  note = {Accessed: 2018-01-02}
}

@misc{wikipedia_sgd,
  title = {Stochastic gradient descent},
  author={Various},
  howpublished = {\url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}},
  note = {Accessed: 2018-01-02}
}

@misc{deeplearning4j,
  title = {Introduction to Deep Neural Networks},
  author={{Deeplearning4j Development Team}},
  publisher={Open-source distributed deep learning for the JVM, Apache Software Foundation License 2.0},
  howpublished = {\url{http://deeplearning4j.org}},
  note = {Accessed: 2018-01-02}
}

@Inbook{Kwolek2005,
author="Kwolek, Bogdan",
editor="Duch, W{\l}odzis{\l}aw
and Kacprzyk, Janusz
and Oja, Erkki
and Zadro{\.{z}}ny, S{\l}awomir",
title="Face Detection Using Convolutional Neural Networks and Gabor Filters",
bookTitle="Artificial Neural Networks: Biological Inspirations -- ICANN 2005: 15th International Conference, Warsaw, Poland, September 11-15, 2005. Proceedings, Part I",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="551--556",
abstract="This paper proposes a method for detecting facial regions by combining a Gabor filter and a convolutional neural network. The first stage uses the Gabor filter which extracts intrinsic facial features. As a result of this transformation we obtain four subimages. The second stage of the method concerns the application of the convolutional neural network to these four images. The approach presented in this paper yields better classification performance in comparison to the results obtained by the convolutional neural network alone.",
isbn="978-3-540-28754-4",
doi="10.1007/11550822_86",
url="https://doi.org/10.1007/11550822_86"
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={1--12},
  year={2017},
  organization={ACM}
}


@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@article{lecun1989bp,
	Annote = {doi: 10.1162/neco.1989.1.4.541},
	Author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	Booktitle = {Neural Computation},
	Da = {1989/12/01},
	Date = {1989/12/01},
	Date-Added = {2016-09-21 14:12:43 +0000},
	Date-Modified = {2016-09-21 14:12:43 +0000},
	Doi = {10.1162/neco.1989.1.4.541},
	Isbn = {0899-7667},
	Journal = {Neural Computation},
	Journal1 = {Neural Computation},
	M3 = {doi: 10.1162/neco.1989.1.4.541},
	Month = {2016/09/21},
	Number = {4},
	Pages = {541--551},
	Publisher = {MIT Press},
	Title = {Backpropagation Applied to Handwritten Zip Code Recognition},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1162/neco.1989.1.4.541},
	Volume = {1},
	Year = {1989},
	Year1 = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/neco.1989.1.4.541}
}

@InProceedings{Taigman_2014_CVPR,
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
title = {DeepFace: Closing the Gap to Human-Level Performance in Face Verification},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
}

@article{tsai2008design,
  title={Design space exploration for 3-D cache},
  author={Tsai, Yuh-Fang and Wang, Feng and Xie, Yuan and Vijaykrishnan, Narayanan and Irwin, Mary Jane},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={16},
  number={4},
  pages={444--455},
  year={2008},
  publisher={IEEE}
}

@article{Hunter:2007,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing In Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python
  for application development, interactive scripting, and
  publication-quality image generation across user
  interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi = {10.1109/MCSE.2007.55},
  year      = 2007
}

@ARTICLE{5604678,
author={G. Van der Plas and P. Limaye and I. Loi and A. Mercha and H. Oprins and C. Torregiani and S. Thijs and D. Linten and M. Stucchi and G. Katti and D. Velenis and V. Cherman and B. Vandevelde and V. Simons and I. De Wolf and R. Labie and D. Perry and S. Bronckers and N. Minas and M. Cupac and W. Ruythooren and J. Van Olmen and A. Phommahaxay and M. de Potter de ten Broeck and A. Opdebeeck and M. Rakowski and B. De Wachter and M. Dehan and M. Nelis and R. Agarwal and A. Pullini and F. Angiolini and L. Benini and W. Dehaene and Y. Travaly and E. Beyne and P. Marchal},
journal={IEEE Journal of Solid-State Circuits},
title={Design Issues and Considerations for Low-Cost 3-D TSV IC Technology},
year={2011},
volume={46},
number={1},
pages={293-307},
keywords={MIS devices;circuit layout;electrostatic discharge;integrated circuit design;integrated circuit interconnections;integrated circuit reliability;network-on-chip;three-dimensional integrated circuits;3D Cu-TSV technology;3D SoC;3D TSV IC technology;3D chip stacks;3D chip-stack;3D network-on-chip;BEOL interconnect reliability;ESD monitoring;MOS devices;RC model;TSV stress;design issues;digital circuit performance;digital gates;mixed signal system performance;noise coupling;thermal floorplanning;thermal hot spot;Arrays;Capacitance;Copper;Electrostatic discharge;Reliability;Stacking;Through-silicon vias;3-D;CU TSV;ESD;mechanical stress;network-on-chip;noise coupling;thermal behavior},
doi={10.1109/JSSC.2010.2074070},
ISSN={0018-9200},
month={Jan},}

@ARTICLE{Iz2005,
author={E. M. Izhikevich},
journal={IEEE Transactions on Neural Networks},
title={Which model to use for cortical spiking neurons?},
year={2004},
volume={15},
number={5},
pages={1063-1070},
keywords={neural nets;neurophysiology;biological plausibility;bursting neurons;computational efficiency;cortical neural networks;cortical spiking neurons;Artificial neural networks;Biological neural networks;Biological system modeling;Computational efficiency;Computational modeling;Fires;Frequency;Information processing;Large-scale systems;Neurons;Action Potentials;Animals;Cerebral Cortex;Humans;Models, Neurological;Nerve Net;Neural Pathways;Neurons;Nonlinear Dynamics;Reaction Time;Synapses;Synaptic Transmission},
doi={10.1109/TNN.2004.832719},
ISSN={1045-9227},
month={Sept},}

@incollection{paugam2012computing,
  title={Computing with spiking neuron networks},
  author={Paugam-Moisy, H{\'e}lene and Bohte, Sander},
  booktitle={Handbook of natural computing},
  pages={335--376},
  year={2012},
  publisher={Springer}
}

@Inbook{Han1995,
author="Han, Jun
and Moraga, Claudio",
editor="Mira, Jos{\'e}
and Sandoval, Francisco",
title="The influence of the sigmoid function parameters on the speed of backpropagation learning",
bookTitle="From Natural to Artificial Neural Computation: International Workshop on Artificial Neural Networks Malaga-Torremolinos, Spain, June 7--9, 1995 Proceedings",
year="1995",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="195--201",
abstract="Sigmoid function is the most commonly known function used in feed forward neural networks because of its nonlinearity and the computational simplicity of its derivative. In this paper we discuss a variant sigmoid function with three parameters that denote the dynamic range, symmetry and slope of the function respectively. We illustrate how these parameters influence the speed of backpropagation learning and introduce a hybrid sigmoidal network with different parameter configuration in different layers. By regulating and modifying the sigmoid function parameter configuration in different layers the error signal problem, oscillation problem and asymmetrical input problem can be reduced. To compare the learning capabilities and the learning rate of the hybrid sigmoidal networks with the conventional networks we have tested the two-spirals benchmark that is known to be a very difficult task for backpropagation and their relatives.",
isbn="978-3-540-49288-7",
doi="10.1007/3-540-59497-3_175",
url="https://doi.org/10.1007/3-540-59497-3_175"
}

@Article{Brunel2007,
author="Brunel, Nicolas
and van Rossum, Mark C. W.",
title="Lapicque's 1907 paper: from frogs to integrate-and-fire",
journal="Biological Cybernetics",
year="2007",
month="Dec",
day="01",
volume="97",
number="5",
pages="337--339",
abstract="Exactly 100 years ago, Louis Lapicque published a paper on the excitability of nerves that is often cited in the context of integrate-and-fire neurons. We discuss Lapicque's contributions along with a translation of the original publication.",
issn="1432-0770",
doi="10.1007/s00422-007-0190-0",
url="https://doi.org/10.1007/s00422-007-0190-0"
}


@ARTICLE{10.3389/fnsys.2015.00151,
AUTHOR={Brette, Romain},   
TITLE={Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain},      
JOURNAL={Frontiers in Systems Neuroscience},      
VOLUME={9},      
PAGES={151},     
YEAR={2015},      
URL={https://www.frontiersin.org/article/10.3389/fnsys.2015.00151},       
DOI={10.3389/fnsys.2015.00151},      
ISSN={1662-5137},   
ABSTRACT={Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.}
}

@INPROCEEDINGS{Kim2016,
author={S. W. Kim and M. Detalle and L. Peng and P. Nolmans and N. Heylen and D. Velenis and A. Miller and G. Beyer and E. Beyne},
booktitle={2016 IEEE 66th Electronic Components and Technology Conference (ECTC)},
title={Ultra-Fine Pitch 3D Integration Using Face-to-Face Hybrid Wafer Bonding Combined with a Via-Middle Through-Silicon-Via Process},
year={2016},
volume={},
number={},
pages={1179-1185},
keywords={annealing;chemical mechanical polishing;copper;dielectric materials;integrated circuit interconnections;three-dimensional integrated circuits;wafer bonding;μbump interconnect;CMP process;Cu;TSV;chemical-mechanical-polishing process;copper damascene patterned surface bonding;dielectric bonding;dielectric damascene surface;face-to-face hybrid wafer bonding;hybrid W2W bonding;hybrid wafer-to-wafer bonding;interconnect density;prebonding wafer treatment;size 1.8 mum;size 3.6 mum;size 30 mm;size 5 mum;temperature 250 C;temperature anneal;ultrafine pitch 3D integration;via-middle through-silicon-via process;void-free room temperature bonding;Bonding;Dielectrics;Stacking;Surface topography;Surface treatment;Through-silicon vias;Wafer bonding;3D integration;TSV;hybrid wafer bonding;low temperature bonding;wafer level stacking;wafer to wafer bonding},
doi={10.1109/ECTC.2016.205},
ISSN={},
month={May},}

@misc{itrs2015_interconn,
title={International Technology Roadmap for Semiconductors 2.0, Interconnect},
url={http://www.itrs2.net/itrs-reports.html},
author={ITRS},
  year={2015},
}

@book{carnevale2006neuron,
  title={The NEURON Book},
  author={Carnevale, N.T. and Hines, M.L.},
  isbn={9781139447836},
  url={https://books.google.com/books?id=YzcOyjKBPHgC},
  year={2006},
  publisher={Cambridge University Press}
}

@book{Jacob:2007:MSC:1543376,
 author = {Jacob, Bruce and Ng, Spencer and Wang, David},
 title = {Memory Systems: Cache, DRAM, Disk},
 year = {2007},
 isbn = {0123797519, 9780123797513},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@ARTICLE{Bamberg2017,
author={L. Bamberg and A. Garcia-Ortiz},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
title={High-Level Energy Estimation for Submicrometric TSV Arrays},
year={2017},
volume={25},
number={10},
pages={2856-2866},
keywords={CMOS integrated circuits;encoding;energy consumption;integrated circuit modelling;low-power electronics;regression analysis;three-dimensional integrated circuits;3D integration;CMOS technologies;MOS capacitances;TSV energy consumption;TSV energy model;bit probabilities;capacitive coupling;coding efficiency;encoder complexity;high-level energy estimation;interconnect delay problem;low-power codes;pattern-dependent energy consumption;planar metal wires;regression method;size 65 nm;submicrometric TSV arrays;temporal misalignment;through silicon vias;voltage-dependent metal-oxide-semiconductor;Capacitance;Couplings;Encoding;Energy consumption;Integrated circuit modeling;Substrates;Through-silicon vias;3-D integrated circuits (3-D ICs);3-D integration;high-level power/energy estimation;low-power coding;through silicon vias (TSVs)},
doi={10.1109/TVLSI.2017.2713601},
ISSN={1063-8210},
month={Oct},}

