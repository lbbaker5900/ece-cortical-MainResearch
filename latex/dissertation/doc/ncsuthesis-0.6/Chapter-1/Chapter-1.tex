%% Lee
%% In dissertation, change 
%    section* to chapter 
%    subsection* to section
%    subsubsection* to subsection

\chapter{Introduction}
%\section{Introduction}
\label{sec:chap-one}


\section{Overview}
\label{sec:overview}

Machine Learning in the form of \acfp{dnn} have gained traction over the last few years.
They get good press in applications such as image recognition and speech recognition.
\acp{dnn} are constructed from a basic building block, the \acf{an}.
With popular \acp{dnn}, the \acf{ann} is often formed from tens of layers with each layer containing many \acp{an}.
In most cases, these layers are processed in a feed-forward manner with one layer being the inputs to the next layer.
Therefore, useful \acp{dnn} often require hundreds of thousands of \acp{an} and within the network, each \ac{an} can have hundreds, even thousands of feeder or pre-synaptic \acp{an}.

There have been implementations that use different number formats from double precision floating point to eight bit integers, but in all cases these useful \ac{ann}s have significant
memory requirements to store the connection weights (parameters) therefore requiring Dynamic Random Access memory (\ac{dram}) to store the \ac{an} parameters.

There have been many successful attempts to accelerate \ac{ann}s, but in most cases the focus is on a subset of the \ac{dnn} known as the Convolutional Neural network (\ac{cnn}).
\acp{cnn} assume a significant amout of reuse of the weights connecting \acp{an} and thus they can take advantage of local memory (\ac{sram}).

Much of the \ac{asic} and \ac{asip} \ac{ann} research has focused on taking advantage of the performance and ease of use of Static Random Access Memory (\ac{sram}).
These implementations can be shown to be effective with specific \ac{ann} architectures, such as \ac{cnn}s where the \ac{ann} parameters can be stored in \ac{sram} in a cache-like architecture avoiding constant accessing of the "slower" \ac{dram}.
In addition, to achieve a high performance, these rely on processing a batch of inputs, such as processing a batch of images or voice recordings using the same \ac{ann}.

The work in this paper considers "edge" applications that require the processing of a disparate set of useful sized \ac{ann}s. The work assumes that the application system is utilizing \ac{ann}s
for the processing of various sub-systems, such as navigation, engine monitoring etc.. This work also does not assume the \ac{ann} is specifically a \ac{cnn} but a \ac{dnn} where there may
not be opportuities to store and reuse portions of the \ac{ann} in \ac{sram}. A further assumption is that the target edge devices do not include opportunities to perform batch processing.
Under these circumstance, when these implemenations need to contantly load \ac{ann} parameters directly from main memory, the performance is constrained to the \ac{dram} interface bandwidth. Therefore, the performance of \ac{sram}-based ASIC/ASIP implementations are severely degraded to the point of being unacceptable.

This work uses the \ac{dram} as the primary processing storage and employs minimal \ac{sram} for the processing of the \ac{an}.
In additon, the work considers 3D integrated circuit technology and a custom 3D-DRAM. By employing \ac{3dic} technology, this work takes advantage of the reduced energy and area and increased
connectivity and bandwidth to allow the \ac{dram} to be employed efficiently without the need for local \ac{sram}.
This work demonstrates that a \ac{3dic} system based on a customized 3D-DRAM could be used in edge applications requiring at or near real-time performance for systems running multiple \ac{ann}s.

It should be noted that this work does not design a custom 3D-DRAM but answers the question "if such a device were available, can we employ it within a useful \ac{ann} system".

%%---------------------------------------------------------------------------------------------------------
%%---------------------------------------------------------------------------------------------------------


\section[Abbreviations]{Abbreviations}
\label{sec:Abbreviations}

\printacronyms[include-classes=abbrev,name={Acronyms}]
%\printacronyms[include-classes=abbrev}

